{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1a85ef",
   "metadata": {},
   "source": [
    "## Test GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8b48a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->99940\n",
      "Process ID: 1343550\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = f\"\"\"bash examples/interactive-LLM/run_qwen2.5-0.5b_gsm8k_multiturn_w_interaction.sh\"\"\"\n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fe99c",
   "metadata": {},
   "source": [
    "## Test MATH-CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7201883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->2147\n",
      "Process ID: 2328007\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = f\"\"\"bash examples/interactive-LLM/run_qwen2.5-7b_math_chat_multiturn_w_interaction.sh\"\"\"\n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554c79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f sglang\n",
    "!pkill -f ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd256941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 169\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/chenxin/verl-interactive/compute_score_log/compute_score_log_LLM_GRPO.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Total lines: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "186b1c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest positive integer \\( k \\) such that the square of the largest prime with 2010 digits minus \\( k \\) is divisible by 12 is \\(\\boxed{1}\\).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.loads(lines[0])['llm_contents'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e53da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 92582\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/chenxin/verl-interactive/compute_score_log/compute_score_log.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Total lines: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a381f183",
   "metadata": {},
   "source": [
    "## Test Token Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae092ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data1/HF-Models/Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "import json\n",
    "\n",
    "token_count = []\n",
    "with open(\"/home/chenxin/verl-interactive/verl-tool/verl/verl_step_records/qwen2.5-0.5b_function_rm-gsm8k-sgl-multi-w-interaction/23.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():                     \n",
    "            item = json.loads(line)     \n",
    "            output = item['output']     \n",
    "            token_length = len(tokenizer.tokenize(output))\n",
    "            token_count.append(token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c83b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     16\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease reason step by step, and put your final answer within \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mboxed\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     18\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     19\u001b[0m ]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# # TIR\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# messages = [\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     {\"role\": \"system\", \"content\": \"Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.\"},\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     {\"role\": \"user\", \"content\": prompt}\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer([text], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# generated_ids = model.generate(\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     **model_inputs,\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#     max_new_tokens=512\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/verl-tool-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1629\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1627\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1629\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1632\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1634\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/verl-tool-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1822\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1823\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1824\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1825\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1826\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1827\u001b[0m         )\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/data1/HF-Models/Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$.\"\n",
    "\n",
    "# CoT\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# # TIR\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.\"},\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     **model_inputs,\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f9f7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n<|im_start|>user\\nFind the value of $x$ that satisfies the equation $4x+5 = 6x+7$.<|im_end|>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl-tool-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
