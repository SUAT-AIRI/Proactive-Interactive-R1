{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "316f60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->90220\n",
      "Process ID: 1486878\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time \n",
    "\n",
    "command = f\"\"\"python -m generalization_eval.miss_premise_testing.run_mip_interactive_generation \\\n",
    "    --input_file \"/home/chenxin/verl-interactive/datasets/gsm8k.json\" \\\n",
    "    --model_url \"http://10.10.128.132:1136\" \\\n",
    "    --model_name Proactive-Interactive-R1-Math-7B \\\n",
    "    --reasoning_model \\\n",
    "    --system_prompt \\\n",
    "    --user_simulator_url \"http://10.10.128.132:8725/v1/\" \\\n",
    "    --question_key \"insufficient_question\" \\\n",
    "    --user_simulator_name Llama-3.1-8B-Instruct \\\n",
    "    --api_key sk-1vfcAowGgLnuRcph74Bc349932Dc40C0Af9eDc007eD0785e\"\"\"\n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2911faa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->31931\n",
      "Process ID: 1847139\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time \n",
    "\n",
    "command = f\"\"\"python -m generalization_eval.miss_premise_testing.math_answer_evaluator \\\n",
    "    --dataset_path /home/chenxin/verl-interactive/results/mip_a800/Proactive-Interactive-R1-Math-7B_gpt-4o-mini_gsm8k_insufficient_question_interactive_generation_result.json \\\n",
    "    --tokenizer_path /data1/HF-Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "    --use_llm_judge \\\n",
    "    --reasoning_model\"\"\"\n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b28231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> some reasoning final answer\n"
     ]
    }
   ],
   "source": [
    "a = \"<think> some reasoning final answer\"\n",
    "print(a.split(\"</think>\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d950fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->49250\n",
      "Process ID: 1844799\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time \n",
    "# user simulator\n",
    "command = f\"\"\"python -m generalization_eval.factual_knowledge.run_mmlu_interactive_generation \\\n",
    "    --data_dir /home/chenxin/verl-interactive/datasets/mmlu/data \\\n",
    "    --model_url \"http://10.10.128.132:1136\" \\\n",
    "    --model_name Proactive-Interactive-R1-Math-7B \\\n",
    "    --user_simulator_url http://10.10.128.132:8725/v1/ \\\n",
    "    --user_simulator_name Llama-3.1-8B-Instruct \\\n",
    "    --save_dir results/mmlu/eval_results/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_generation \\\n",
    "    --max_workers 32\"\"\"\n",
    "    \n",
    "    \n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8306752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f generalization_eval.factual_knowledge.run_mmlu_pro_interactive_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e410c044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->7051\n",
      "Process ID: 1666557\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time \n",
    "# user simulator\n",
    "command = f\"\"\"python -m generalization_eval.factual_knowledge.run_mmlu_pro_interactive_generation \\\n",
    "    --data_dir /home/chenxin/verl-interactive/datasets/TIGER-Lab/MMLU-Pro \\\n",
    "    --output_dir /home/chenxin/verl-interactive/results/MMLU_Pro/eval_results/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_result \\\n",
    "    --model_url \"http://10.10.128.132:1136\" \\\n",
    "    --model_name Proactive-Interactive-R1-Math-7B \\\n",
    "    --user_simulator_url http://10.10.128.132:8725/v1/ \\\n",
    "    --user_simulator_name Llama-3.1-8B-Instruct \\\n",
    "    --num_workers 32\"\"\"\n",
    "    \n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011888bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->33950\n",
      "Process ID: 1876893\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time \n",
    "# user simulator\n",
    "command = f\"\"\"python -m generalization_eval.retrieval_question_answering.run_retriaqa_interactive_generation \\\n",
    "    --data_dir /home/chenxin/verl-interactive/datasets/mandarjoshi/trivia_qa \\\n",
    "    --model_url \"http://10.10.128.132:1136\" \\\n",
    "    --model_name Proactive-Interactive-R1-Math-7B \\\n",
    "    --reasoning_model \\\n",
    "    --user_simulator_url http://10.10.128.132:8725/v1/ \\\n",
    "    --user_simulator_name Llama-3.1-8B-Instruct \\\n",
    "    --output_dir /home/chenxin/verl-interactive/results/retriaqa/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_generation \\\n",
    "    --num_workers 32\"\"\"\n",
    "    \n",
    "    \n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426efe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f generalization_eval.retrieval_question_answering.run_retriaqa_interactive_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd273b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->56834\n",
      "Process ID: 1872453\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time \n",
    "# user simulator\n",
    "command = f\"\"\"python -m generalization_eval.retrieval_question_answering.run_squad_interactive_generation \\\n",
    "    --data_dir /home/chenxin/verl-interactive/datasets/rajpurkar/squad \\\n",
    "    --model_url \"http://10.10.128.132:1136\" \\\n",
    "    --model_name Proactive-Interactive-R1-Math-7B \\\n",
    "    --reasoning_model \\\n",
    "    --user_simulator_url http://10.10.128.132:8725/v1/ \\\n",
    "    --user_simulator_name Llama-3.1-8B-Instruct \\\n",
    "    --output_dir /home/chenxin/verl-interactive/results/squad/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_generation \\\n",
    "    --num_workers 16\"\"\"\n",
    "    \n",
    "    \n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ec1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f generalization_eval.retrieval_question_answering.run_squad_interactive_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66532ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_number->85139\n",
      "Process ID: 1885769\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time \n",
    "# user simulator\n",
    "command = f\"\"\"LLM_JUDGEMENT_API_BASE=https://api.ai-gaochao.cn/v1/ LLM_JUDGEMENT_API_KEY=\"sk-EAR9W8gMRCM2JIMJ2e5b8b095c494718B5CeC1C17004577d\" LLM_JUDGEMENT_MODEL=\"gpt-4o-mini\" python -m generalization_eval.retrieval_question_answering.retriqa_evaluator \\\n",
    "    --input_file /home/chenxin/verl-interactive/results/retriaqa/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_generation/triviaqa_results_Proactive-Interactive-R1-Math-7B.jsonl \\\n",
    "    --tokenizer_path /data1/HF-Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "    --use_llm_judge\"\"\"\n",
    "    \n",
    "import random\n",
    "random_number = random.randint(1, 100000)\n",
    "print(f\"random_number->{random_number}\")\n",
    "with open(f\"logs/output_{random_number}.log\", \"w\") as outfile:\n",
    "    process = subprocess.Popen(command, shell=True, stdout=outfile, stderr=subprocess.STDOUT)\n",
    "print(f\"Process ID: {process.pid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a9dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 14 subject files...\n",
      "\n",
      "==================================================\n",
      "MMLU-Pro Detailed Evaluation Results\n",
      "==================================================\n",
      "| Subject          |   Correct |   Total | Accuracy   |\n",
      "|:-----------------|----------:|--------:|:-----------|\n",
      "| biology          |       441 |     717 | 61.51%     |\n",
      "| business         |       485 |     789 | 61.47%     |\n",
      "| chemistry        |       694 |    1132 | 61.31%     |\n",
      "| computer science |       255 |     410 | 62.20%     |\n",
      "| economics        |       544 |     844 | 64.45%     |\n",
      "| engineering      |       376 |     969 | 38.80%     |\n",
      "| health           |       292 |     818 | 35.70%     |\n",
      "| history          |       102 |     381 | 26.77%     |\n",
      "| law              |       218 |    1101 | 19.80%     |\n",
      "| math             |      1003 |    1351 | 74.24%     |\n",
      "| other            |       347 |     924 | 37.55%     |\n",
      "| philosophy       |       174 |     499 | 34.87%     |\n",
      "| physics          |       814 |    1299 | 62.66%     |\n",
      "| psychology       |       397 |     798 | 49.75%     |\n",
      "\n",
      "\n",
      "==================================================\n",
      "Overall Accuracy: 51.05% (6142/12032)\n",
      "==================================================\n",
      "\n",
      "Results saved to /home/chenxin/verl-interactive/results/MMLU_Pro/eval_results/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_result/evaluation_summary.csv\n",
      "Results saved to /home/chenxin/verl-interactive/results/MMLU_Pro/eval_results/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_result/evaluation_summary.json\n",
      "Results saved to /home/chenxin/verl-interactive/results/MMLU_Pro/eval_results/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_result/evaluation_summary.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_results(output_dir, save_path=None):\n",
    "    pattern = os.path.join(output_dir, \"*_result.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No result files found in directory {output_dir}.\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    print(f\"Analyzing {len(files)} subject files...\\n\")\n",
    "\n",
    "    for file_path in files:\n",
    "        subject_name = os.path.basename(file_path).replace(\"_result.json\", \"\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read file {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        subject_total = len(data)\n",
    "        subject_correct = 0\n",
    "        \n",
    "        for item in data:\n",
    "            pred = item.get(\"pred\")\n",
    "            label = item.get(\"answer\")\n",
    "            if pred and pred == label:\n",
    "                subject_correct += 1\n",
    "        \n",
    "        if subject_total > 0:\n",
    "            acc = subject_correct / subject_total\n",
    "        else:\n",
    "            acc = 0.0\n",
    "\n",
    "        results.append({\n",
    "            \"Subject\": subject_name,\n",
    "            \"Correct\": subject_correct,\n",
    "            \"Total\": subject_total,\n",
    "            \"Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        total_correct += subject_correct\n",
    "        total_count += subject_total\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values(by=\"Subject\")\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    if total_count > 0:\n",
    "        overall_acc = total_correct / total_count\n",
    "    else:\n",
    "        overall_acc = 0.0\n",
    "\n",
    "    # Print results\n",
    "    print(\"=\"*50)\n",
    "    print(\"MMLU-Pro Detailed Evaluation Results\")\n",
    "    print(\"=\"*50)\n",
    "    df_display = df.copy()\n",
    "    df_display[\"Accuracy\"] = df_display[\"Accuracy\"].apply(lambda x: f\"{x:.2%}\")\n",
    "    print(df_display.to_markdown(index=False)) \n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall Accuracy: {overall_acc:.2%} ({total_correct}/{total_count})\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Save results to file\n",
    "    if save_path is None:\n",
    "        save_path = os.path.join(output_dir, \"evaluation_summary\")\n",
    "    \n",
    "    # Method 1: Save as CSV\n",
    "    df.to_csv(f\"{save_path}.csv\", index=False)\n",
    "    print(f\"\\nResults saved to {save_path}.csv\")\n",
    "    \n",
    "    # Method 2: Save as JSON\n",
    "    summary = {\n",
    "        \"subjects\": results,\n",
    "        \"overall\": {\n",
    "            \"total_correct\": total_correct,\n",
    "            \"total_count\": total_count,\n",
    "            \"accuracy\": overall_acc\n",
    "        }\n",
    "    }\n",
    "    with open(f\"{save_path}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Results saved to {save_path}.json\")\n",
    "    \n",
    "    # Method 3: Save as TXT\n",
    "    with open(f\"{save_path}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"MMLU-Pro Detailed Evaluation Results\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(df_display.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Overall Accuracy: {overall_acc:.2%} ({total_correct}/{total_count})\\n\")\n",
    "    print(f\"Results saved to {save_path}.txt\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Usage\n",
    "result = analyze_results(\n",
    "    \"/home/chenxin/verl-interactive/results/MMLU_Pro/eval_results/Proactive-Interactive-R1-Math-7B_Llama-3.1-8B-Instruct_interactive_result\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl-tool-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
